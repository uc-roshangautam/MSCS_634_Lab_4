{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad9e3cf4",
   "metadata": {},
   "source": [
    "\n",
    " MSCS_634_Lab_4\n",
    " Roshan Gautam\n",
    " University of Cumberlands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b198db9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Step 1: Data Preparation\n",
    "print(\"=== STEP 1: DATA PREPARATION ===\")\n",
    "diabetes = load_diabetes()\n",
    "X, y = diabetes.data, diabetes.target\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "feature_names = diabetes.feature_names\n",
    "X_df = pd.DataFrame(X, columns=feature_names)\n",
    "y_df = pd.Series(y, name='target')\n",
    "\n",
    "print(f\"Dataset shape: {X_df.shape}\")\n",
    "print(f\"Target shape: {y_df.shape}\")\n",
    "print(f\"\\nFeature names: {list(feature_names)}\")\n",
    "print(f\"\\nTarget statistics:\")\n",
    "print(y_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values in features: {X_df.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in target: {y_df.isnull().sum()}\")\n",
    "\n",
    "# Data distribution\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    axes[i].hist(X_df[feature], bins=20, alpha=0.7)\n",
    "    axes[i].set_title(f'{feature}')\n",
    "    axes[i].set_xlabel('Value')\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Target distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(y, bins=20, alpha=0.7)\n",
    "plt.title('Target Distribution')\n",
    "plt.xlabel('Target Value')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(y)\n",
    "plt.title('Target Boxplot')\n",
    "plt.ylabel('Target Value')\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = X_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Step 2: Linear Regression (Simple)\n",
    "print(\"\\n=== STEP 2: SIMPLE LINEAR REGRESSION ===\")\n",
    "\n",
    "# Use the feature with highest correlation to target\n",
    "correlations = X_df.corrwith(y_df)\n",
    "best_feature_idx = np.abs(correlations).idxmax()\n",
    "print(f\"Selected feature for simple regression: {best_feature_idx}\")\n",
    "print(f\"Correlation with target: {correlations[best_feature_idx]:.3f}\")\n",
    "\n",
    "X_simple = X_df[[best_feature_idx]]\n",
    "X_train_simple, X_test_simple, y_train, y_test = train_test_split(\n",
    "    X_simple, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train simple linear regression\n",
    "lr_simple = LinearRegression()\n",
    "lr_simple.fit(X_train_simple, y_train)\n",
    "y_pred_simple = lr_simple.predict(X_test_simple)\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred, model_name):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"{model_name} Metrics:\")\n",
    "    print(f\"  MAE: {mae:.3f}\")\n",
    "    print(f\"  MSE: {mse:.3f}\")\n",
    "    print(f\"  RMSE: {rmse:.3f}\")\n",
    "    print(f\"  R²: {r2:.3f}\")\n",
    "    \n",
    "    return {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'R2': r2}\n",
    "\n",
    "simple_metrics = calculate_metrics(y_test, y_pred_simple, \"Simple Linear Regression\")\n",
    "\n",
    "# Visualize simple regression\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test_simple, y_test, alpha=0.6, label='Actual')\n",
    "plt.scatter(X_test_simple, y_pred_simple, alpha=0.6, label='Predicted')\n",
    "plt.xlabel(best_feature_idx)\n",
    "plt.ylabel('Target')\n",
    "plt.title('Simple Linear Regression: Actual vs Predicted')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 3: Multiple Regression\n",
    "print(\"\\n=== STEP 3: MULTIPLE REGRESSION ===\")\n",
    "\n",
    "X_train_multi, X_test_multi, y_train_multi, y_test_multi = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Train multiple regression\n",
    "lr_multi = LinearRegression()\n",
    "lr_multi.fit(X_train_multi, y_train_multi)\n",
    "y_pred_multi = lr_multi.predict(X_test_multi)\n",
    "\n",
    "multi_metrics = calculate_metrics(y_test_multi, y_pred_multi, \"Multiple Linear Regression\")\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'coefficient': lr_multi.coef_\n",
    "}).sort_values('coefficient', key=abs, ascending=False)\n",
    "\n",
    "print(f\"\\nFeature Importance (Coefficients):\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualize multiple regression\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test_multi, y_pred_multi, alpha=0.6)\n",
    "plt.plot([y_test_multi.min(), y_test_multi.max()], [y_test_multi.min(), y_test_multi.max()], 'r--', lw=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title('Multiple Linear Regression: Actual vs Predicted')\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Polynomial Regression\n",
    "print(\"\\n=== STEP 4: POLYNOMIAL REGRESSION ===\")\n",
    "\n",
    "# Test different polynomial degrees\n",
    "degrees = [2, 3, 4]\n",
    "poly_results = {}\n",
    "\n",
    "for degree in degrees:\n",
    "    print(f\"\\nPolynomial Degree {degree}:\")\n",
    "    \n",
    "    # Create polynomial features\n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly_features.fit_transform(X)\n",
    "    \n",
    "    # Split data\n",
    "    X_train_poly, X_test_poly, y_train_poly, y_test_poly = train_test_split(\n",
    "        X_poly, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train polynomial regression\n",
    "    lr_poly = LinearRegression()\n",
    "    lr_poly.fit(X_train_poly, y_train_poly)\n",
    "    y_pred_poly = lr_poly.predict(X_test_poly)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    poly_metrics = calculate_metrics(y_test_poly, y_pred_poly, f\"Polynomial (degree {degree})\")\n",
    "    poly_results[degree] = {\n",
    "        'model': lr_poly,\n",
    "        'metrics': poly_metrics,\n",
    "        'y_pred': y_pred_poly,\n",
    "        'y_test': y_test_poly\n",
    "    }\n",
    "\n",
    "# Visualize polynomial regression results\n",
    "fig, axes = plt.subplots(1, len(degrees), figsize=(15, 5))\n",
    "for i, degree in enumerate(degrees):\n",
    "    result = poly_results[degree]\n",
    "    axes[i].scatter(result['y_test'], result['y_pred'], alpha=0.6)\n",
    "    axes[i].plot([result['y_test'].min(), result['y_test'].max()], \n",
    "                 [result['y_test'].min(), result['y_test'].max()], 'r--', lw=2)\n",
    "    axes[i].set_xlabel('Actual Values')\n",
    "    axes[i].set_ylabel('Predicted Values')\n",
    "    axes[i].set_title(f'Polynomial Degree {degree}\\nR² = {result[\"metrics\"][\"R2\"]:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Ridge and Lasso Regression\n",
    "print(\"\\n=== STEP 5: REGULARIZATION (RIDGE & LASSO) ===\")\n",
    "\n",
    "# Test different alpha values\n",
    "alphas = [0.1, 1.0, 10.0, 100.0]\n",
    "ridge_results = {}\n",
    "lasso_results = {}\n",
    "\n",
    "print(\"Ridge Regression:\")\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha, random_state=42)\n",
    "    ridge.fit(X_train_multi, y_train_multi)\n",
    "    y_pred_ridge = ridge.predict(X_test_multi)\n",
    "    \n",
    "    ridge_metrics = calculate_metrics(y_test_multi, y_pred_ridge, f\"Ridge (α={alpha})\")\n",
    "    ridge_results[alpha] = {\n",
    "        'model': ridge,\n",
    "        'metrics': ridge_metrics,\n",
    "        'y_pred': y_pred_ridge\n",
    "    }\n",
    "    print()\n",
    "\n",
    "print(\"Lasso Regression:\")\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha, random_state=42, max_iter=2000)\n",
    "    lasso.fit(X_train_multi, y_train_multi)\n",
    "    y_pred_lasso = lasso.predict(X_test_multi)\n",
    "    \n",
    "    lasso_metrics = calculate_metrics(y_test_multi, y_pred_lasso, f\"Lasso (α={alpha})\")\n",
    "    lasso_results[alpha] = {\n",
    "        'model': lasso,\n",
    "        'metrics': lasso_metrics,\n",
    "        'y_pred': y_pred_lasso\n",
    "    }\n",
    "    print()\n",
    "\n",
    "# Visualize regularization effects\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Ridge regression comparison\n",
    "axes[0, 0].plot(alphas, [ridge_results[a]['metrics']['R2'] for a in alphas], 'o-', label='Ridge')\n",
    "axes[0, 0].plot(alphas, [lasso_results[a]['metrics']['R2'] for a in alphas], 's-', label='Lasso')\n",
    "axes[0, 0].set_xscale('log')\n",
    "axes[0, 0].set_xlabel('Alpha')\n",
    "axes[0, 0].set_ylabel('R² Score')\n",
    "axes[0, 0].set_title('R² Score vs Alpha')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].plot(alphas, [ridge_results[a]['metrics']['RMSE'] for a in alphas], 'o-', label='Ridge')\n",
    "axes[0, 1].plot(alphas, [lasso_results[a]['metrics']['RMSE'] for a in alphas], 's-', label='Lasso')\n",
    "axes[0, 1].set_xscale('log')\n",
    "axes[0, 1].set_xlabel('Alpha')\n",
    "axes[0, 1].set_ylabel('RMSE')\n",
    "axes[0, 1].set_title('RMSE vs Alpha')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Best Ridge and Lasso predictions\n",
    "best_ridge_alpha = min(alphas, key=lambda a: ridge_results[a]['metrics']['RMSE'])\n",
    "best_lasso_alpha = min(alphas, key=lambda a: lasso_results[a]['metrics']['RMSE'])\n",
    "\n",
    "axes[1, 0].scatter(y_test_multi, ridge_results[best_ridge_alpha]['y_pred'], alpha=0.6)\n",
    "axes[1, 0].plot([y_test_multi.min(), y_test_multi.max()], [y_test_multi.min(), y_test_multi.max()], 'r--', lw=2)\n",
    "axes[1, 0].set_xlabel('Actual Values')\n",
    "axes[1, 0].set_ylabel('Predicted Values')\n",
    "axes[1, 0].set_title(f'Best Ridge (α={best_ridge_alpha})\\nR² = {ridge_results[best_ridge_alpha][\"metrics\"][\"R2\"]:.3f}')\n",
    "\n",
    "axes[1, 1].scatter(y_test_multi, lasso_results[best_lasso_alpha]['y_pred'], alpha=0.6)\n",
    "axes[1, 1].plot([y_test_multi.min(), y_test_multi.max()], [y_test_multi.min(), y_test_multi.max()], 'r--', lw=2)\n",
    "axes[1, 1].set_xlabel('Actual Values')\n",
    "axes[1, 1].set_ylabel('Predicted Values')\n",
    "axes[1, 1].set_title(f'Best Lasso (α={best_lasso_alpha})\\nR² = {lasso_results[best_lasso_alpha][\"metrics\"][\"R2\"]:.3f}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature selection comparison\n",
    "best_ridge_model = ridge_results[best_ridge_alpha]['model']\n",
    "best_lasso_model = lasso_results[best_lasso_alpha]['model']\n",
    "\n",
    "ridge_coefs = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'ridge_coef': best_ridge_model.coef_,\n",
    "    'lasso_coef': best_lasso_model.coef_\n",
    "})\n",
    "\n",
    "print(f\"\\nCoefficient Comparison (Ridge α={best_ridge_alpha}, Lasso α={best_lasso_alpha}):\")\n",
    "print(ridge_coefs.round(3))\n",
    "\n",
    "# Step 6: Model Comparison and Analysis\n",
    "print(\"\\n=== STEP 6: MODEL COMPARISON ===\")\n",
    "\n",
    "# Compile all results\n",
    "all_results = {\n",
    "    'Simple Linear': simple_metrics,\n",
    "    'Multiple Linear': multi_metrics,\n",
    "    'Polynomial (deg=2)': poly_results[2]['metrics'],\n",
    "    'Polynomial (deg=3)': poly_results[3]['metrics'],\n",
    "    'Polynomial (deg=4)': poly_results[4]['metrics'],\n",
    "    f'Ridge (α={best_ridge_alpha})': ridge_results[best_ridge_alpha]['metrics'],\n",
    "    f'Lasso (α={best_lasso_alpha})': lasso_results[best_lasso_alpha]['metrics']\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(all_results).T\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_df.round(3))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "metrics = ['MAE', 'MSE', 'RMSE', 'R2']\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    values = comparison_df[metric]\n",
    "    bars = ax.bar(range(len(values)), values)\n",
    "    ax.set_xticks(range(len(values)))\n",
    "    ax.set_xticklabels(values.index, rotation=45, ha='right')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    \n",
    "    # Highlight best performance\n",
    "    if metric == 'R2':\n",
    "        best_idx = values.idxmax()\n",
    "        best_pos = list(values.index).index(best_idx)\n",
    "        bars[best_pos].set_color('gold')\n",
    "    else:\n",
    "        best_idx = values.idxmin()\n",
    "        best_pos = list(values.index).index(best_idx)\n",
    "        bars[best_pos].set_color('gold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary insights\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "best_r2_model = comparison_df['R2'].idxmax()\n",
    "best_rmse_model = comparison_df['RMSE'].idxmin()\n",
    "\n",
    "print(f\"Best R² Score: {best_r2_model} ({comparison_df.loc[best_r2_model, 'R2']:.3f})\")\n",
    "print(f\"Best RMSE: {best_rmse_model} ({comparison_df.loc[best_rmse_model, 'RMSE']:.3f})\")\n",
    "\n",
    "print(f\"\\nPerformance Analysis:\")\n",
    "print(f\"1. Multiple regression significantly outperformed simple regression\")\n",
    "print(f\"2. Polynomial regression showed {'improvement' if comparison_df.loc['Polynomial (deg=2)', 'R2'] > comparison_df.loc['Multiple Linear', 'R2'] else 'mixed results'} over linear models\")\n",
    "print(f\"3. Regularization helped {'control overfitting' if best_r2_model in ['Ridge', 'Lasso'] else 'but did not improve over polynomial models'}\")\n",
    "print(f\"4. Lasso regression selected {np.sum(best_lasso_model.coef_ != 0)} out of {len(feature_names)} features\")\n",
    "\n",
    "print(f\"\\nDataset Insights:\")\n",
    "print(f\"- Features show moderate correlations with target variable\")\n",
    "print(f\"- Most important feature: {feature_importance.iloc[0]['feature']}\")\n",
    "print(f\"- Regularization revealed feature importance patterns\")\n",
    "print(f\"- Model complexity vs performance trade-offs observed\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
